{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5 - Deep Learning\n",
    "\n",
    "### <font color=\"red\"> DUE: Apr 19 (Thursday) 11:00 pm </font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Sagar Nandu </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def getAllPreformanceMeasures(output, target):\n",
    "    target = target.flatten() # if target vector is not one dimentional in axis = 1\n",
    "    CM = metrics.confusion_matrix(output, target) #creating confusion matrix\n",
    "    TN = CM[0][0] # True Negatives\n",
    "    FN = CM[1][0] # False Negatives\n",
    "    TP = CM[1][1] # True Positives\n",
    "    FP = CM[0][1] # False Positives\n",
    "    return (TP+TN)*100/output.shape[0],(TP)*100/(TP+FP),(TP)*100/(TP+FN) # Returns (accuracy, precesion and recall)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    return 1 if x > 0 else -1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return x if x > 0 else 0\n",
    "\n",
    "def leak_relu(x):\n",
    "    return x if x > 0 else 0.01 * x\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1+np.exp(x))\n",
    "\n",
    "def gauss(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Overview\n",
    "\n",
    "Describe the objective of this assignment. You can briefly state how you accompilsh it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "- Learn Tensorflow\n",
    "- How differnt types of NN structures act and differnt results\n",
    "- Changing hyper-parameters, what are differnt results\n",
    "- Trying differnt activation fuctions and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data\n",
    "\n",
    "Introduce your data and visualize them. Describe your observations about the data.\n",
    "You can reuse the data that you examined in Assignment #0 (of course for classification). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "\n",
    "There are 25 variables:\n",
    "\n",
    "1. <b>ID:</b> ID of each client  \n",
    "2. <b>LIMIT_BAL:</b> Amount of given credit in NT dollars (includes individual and family/supplementary credit  \n",
    "3. <b>SEX:</b> Gender (1=male, 2=female)  \n",
    "4. <b>EDUCATION:</b> (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)  \n",
    "5. <b>MARRIAGE:</b> Marital status (1=married, 2=single, 3=others)  \n",
    "6. <b>AGE:</b> Age in years  \n",
    "7. <b>PAY_0:</b> Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "8. <b>PAY_2:</b> Repayment status in August, 2005 (scale same as above)  \n",
    "9. <b>PAY_3:</b> Repayment status in July, 2005 (scale same as above)  \n",
    "10. <b>PAY_4:</b> Repayment status in June, 2005 (scale same as above)  \n",
    "11. <b>PAY_5:</b> Repayment status in May, 2005 (scale same as above)  \n",
    "12. <b>PAY_6:</b> Repayment status in April, 2005 (scale same as above)  \n",
    "13. <b>BILL_AMT1:</b> Amount of bill statement in September, 2005 (NT dollar)  \n",
    "14. <b>BILL_AMT2:</b> Amount of bill statement in August, 2005 (NT dollar)  \n",
    "15. <b>BILL_AMT3:</b> Amount of bill statement in July, 2005 (NT dollar)  \n",
    "16. <b>BILL_AMT4:</b> Amount of bill statement in June, 2005 (NT dollar)  \n",
    "17. <b>BILL_AMT5:</b> Amount of bill statement in May, 2005 (NT dollar)  \n",
    "18. <b>BILL_AMT6:</b> Amount of bill statement in April, 2005 (NT dollar)  \n",
    "19. <b>PAY_AMT1:</b> Amount of previous payment in September, 2005 (NT dollar)  \n",
    "20. <b>PAY_AMT2:</b> Amount of previous payment in August, 2005 (NT dollar)  \n",
    "21. <b>PAY_AMT3:</b> Amount of previous payment in July, 2005 (NT dollar)  \n",
    "22. <b>PAY_AMT4:</b> Amount of previous payment in June, 2005 (NT dollar)  \n",
    "23. <b>PAY_AMT5:</b> Amount of previous payment in May, 2005 (NT dollar)  \n",
    "24. <b>PAY_AMT6:</b> Amount of previous payment in April, 2005 (NT dollar)  \n",
    "25. <b>default.payment.next.month:</b> Default payment (1=yes, 0=no)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData = pd.read_csv(\"credit_card_clients.csv\",encoding=\"utf-8\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                         \n",
       "1       20000    2          2         1   24      2      2     -1     -1   \n",
       "2      120000    2          2         2   26     -1      2      0      0   \n",
       "3       90000    2          2         2   34      0      0      0      0   \n",
       "4       50000    2          2         1   37      0      0      0      0   \n",
       "5       50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "    PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "ID         ...                                                                  \n",
       "1      -2  ...          0          0          0         0       689         0   \n",
       "2       0  ...       3272       3455       3261         0      1000      1000   \n",
       "3       0  ...      14331      14948      15549      1518      1500      1000   \n",
       "4       0  ...      28314      28959      29547      2000      2019      1200   \n",
       "5       0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "    PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                            \n",
       "1          0         0         0                           1  \n",
       "2       1000         0      2000                           1  \n",
       "3       1000      1000      5000                           0  \n",
       "4       1100      1069      1000                           0  \n",
       "5       9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuousCols = ['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData[continuousCols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalCols = [\"AGE_RANGE\",\"SEX\",\"EDUCATION\",\"MARRIAGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this\n",
    "for col in categoricalCols:\n",
    "    creditData[col] = creditData[col].astype('category')\n",
    "creditData[categoricalCols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure with subplots of different sizes\n",
    "fig = plt.figure(1)\n",
    "plt.figure(figsize=(15,5))# set up subplot grid\n",
    "gridspec.GridSpec(1,4)\n",
    "plt.subplot2grid((1,4), (0,0), colspan=2, rowspan=1)\n",
    "sns.kdeplot(creditData.loc[creditData[\"default.payment.next.month\"]==0,\"AGE\"].values, label=\"no\")\n",
    "sns.kdeplot(creditData.loc[creditData[\"default.payment.next.month\"]==1,\"AGE\"].values, label=\"yes\")\n",
    "plt.legend();\n",
    "plt.title(\"Age Distribution for two classes\")\n",
    "plt.subplot2grid((1,4), (0,2), colspan=2, rowspan=1)\n",
    "sns.kdeplot(creditData.loc[creditData[\"default.payment.next.month\"]==0,\"LIMIT_BAL\"].values, label=\"no\")\n",
    "sns.kdeplot(creditData.loc[creditData[\"default.payment.next.month\"]==1,\"LIMIT_BAL\"].values, label=\"yes\")\n",
    "plt.ylim([0,0.9])\n",
    "plt.legend();\n",
    "plt.title(\"LIMIT_BAL Distribution for two classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData[['SEX', 'EDUCATION', 'MARRIAGE','default.payment.next.month']].hist(figsize=(15,15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Types:\n",
    "1. Continuous:<br>\n",
    "'LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'\n",
    "2. Categorical: <br>\n",
    "'SEX', 'EDUCATION', 'MARRIAGE'\n",
    "3. Target:<br>\n",
    "'default.payment.next.month'\n",
    "\n",
    "#### Preprocessing steps:\n",
    "1. Normalizing Continuous variables\n",
    "2. One Hot Encoding for Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData.loc[creditData[\"SEX\"]==2,\"SEX\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageGrps = list(range(20,85,5))\n",
    "ageCrediters = pd.cut(creditData.AGE,ageGrps).to_frame()\n",
    "ageCrediters.columns = [\"AGE_RANGE\"]\n",
    "creditData = pd.concat([creditData,ageCrediters],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData.AGE_RANGE = creditData.AGE_RANGE.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(creditData[\"MARRIAGE\"])\n",
    "df.columns = ['MARRIAGE_unk','MARRIAGE_married', 'MARRIAGE_single','MARRIAGE_others']\n",
    "creditData = pd.concat([creditData,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData[\"EDUCATION_grad\"] = 0\n",
    "creditData[\"EDUCATION_university\"] = 0\n",
    "creditData[\"EDUCATION_hs\"] = 0\n",
    "creditData[\"EDUCATION_others\"] = 0\n",
    "creditData[\"EDUCATION_ukn\"] = 0\n",
    "creditData.loc[creditData[\"EDUCATION\"]==1,\"EDUCATION_grad\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==2,\"EDUCATION_university\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==3,\"EDUCATION_hs\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==4,\"EDUCATION_others\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==5,\"EDUCATION_ukn\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==0,\"EDUCATION_ukn\"] = 1\n",
    "creditData.loc[creditData[\"EDUCATION\"]==6,\"EDUCATION_ukn\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(creditData.AGE_RANGE)\n",
    "df.columns = ['AGE_(20, 25]', 'AGE_(25, 30]', 'AGE_(30, 35]', 'AGE_(35, 40]', 'AGE_(40, 45]', 'AGE_(45, 50]','AGE_(50, 55]', 'AGE_(55, 60]', 'AGE_(60, 65]', 'AGE_(65, 70]', 'AGE_(70, 75]', 'AGE_(75, 80]']\n",
    "creditData = pd.concat([creditData,df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "creditData.loc[:, continuousCols] = StandardScaler().fit_transform(creditData.loc[:, continuousCols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LIMIT_BAL', 'PAY_0', 'PAY_2',\n",
    "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'PAY_AMT1',\n",
    "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "       'MARRIAGE_unk', 'MARRIAGE_married',\n",
    "        'EDUCATION_grad',\n",
    "       'EDUCATION_university', 'EDUCATION_others',\n",
    "       'EDUCATION_ukn', 'AGE_(20, 25]', 'AGE_(25, 30]',\n",
    "       'AGE_(30, 35]', 'AGE_(35, 40]', 'AGE_(40, 45]', 'AGE_(45, 50]',\n",
    "       'AGE_(50, 55]', 'AGE_(55, 60]', 'AGE_(60, 65]', 'AGE_(65, 70]',\n",
    "       'AGE_(70, 75]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "heat = sns.heatmap(creditData[['AGE_(20, 25]', 'AGE_(25, 30]',\n",
    "       'AGE_(30, 35]', 'AGE_(35, 40]', 'AGE_(40, 45]', 'AGE_(45, 50]',\n",
    "       'AGE_(50, 55]', 'AGE_(55, 60]', 'AGE_(60, 65]', 'AGE_(65, 70]',\n",
    "       'AGE_(70, 75]', 'AGE_(75, 80]','default.payment.next.month']].corr(), vmax=1, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "heat = sns.heatmap(creditData[features].corr(), vmax=1, square=True, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = creditData[features+[\"default.payment.next.month\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = data.sample(frac=.70) # random sampling of 70 of data\n",
    "testData = data.loc[set(data.index)- set(trainData.index)] # picking the rest 30% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain = trainData[features].values\n",
    "TTrain = trainData[\"default.payment.next.month\"].values\n",
    "XTest = testData[features].values\n",
    "TTest = testData[\"default.payment.next.month\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Methods\n",
    "\n",
    "In this assignment, you are build a deep network with more than 5 layers using TensorFlow. \n",
    "Looking at the chart below, get some idea about how you can construct your networks for what problem and why you pick your structure. \n",
    "\n",
    "- Pick at least 3 different networks for experiments.\n",
    "- Summarize the choice of your networks. \n",
    "- Explain your TensorFlow codes. \n",
    "- Explain how you apply your model to your data. \n",
    "\n",
    "Following images are only for you to get some idea. You do not necessarily stick with these. You can come up with your own structure or shape. \n",
    "\n",
    "![](https://camo.githubusercontent.com/31e20172e3b7bc5530a6e7c7e7339e8e556d0acc/687474703a2f2f7777772e6173696d6f76696e737469747574652e6f72672f77702d636f6e74656e742f75706c6f6164732f323031362f30392f6e657572616c6e6574776f726b732e706e67)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Straight graph based model\n",
    "- 5 hidden + 1 input + 1 output layers\n",
    "- All hidden layers have equal number of nodes\n",
    "- Chose this to understand how tensorflow works and it would be simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 5\n",
    "n_hidden2 = 5\n",
    "n_hidden3 = 5\n",
    "n_hidden4 = 5\n",
    "n_hidden5 = 5\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.tanh, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.tanh, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.tanh, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.tanh, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.tanh, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1000\n",
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increased Number of Iterations:\n",
    "- Improved accuracy\n",
    "- Takes longer\n",
    "- Improvement was not too much, so we have take of the trade of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=3000\n",
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criss Cross and then narrowing\n",
    "- 7 hidden (4 Criss Cross + 3 narrow ) + 1 input + 1 output layers\n",
    "- Chose this structure to see how data expands and contracts (Encode and Decode) effect which later on converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 31\n",
    "n_hidden2 = 16\n",
    "n_hidden3 = 31\n",
    "n_hidden4 = 16\n",
    "n_hidden5 = 8\n",
    "n_hidden6 = 6\n",
    "n_hidden7 = 4\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=selu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=selu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=selu, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=selu, name=\"hidden6\")\n",
    "    hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=selu, name=\"hidden7\")\n",
    "    logits = tf.layers.dense(hidden7, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing number of Iterations to a much significant number when number of layers have increased\n",
    "- Here I see that accuracy remain unchanged\n",
    "- But it takes a lot of time to finish training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "\n",
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Criss Cross \n",
    "- 6 hidden + 1 input + 1 output layers\n",
    "- Chose this structure to see how data expands and contracts (Encode and Decode) effect alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 15\n",
    "n_hidden2 = 5\n",
    "n_hidden3 = 15\n",
    "n_hidden4 = 5\n",
    "n_hidden5 = 15\n",
    "n_hidden6 = 5\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.tanh, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.tanh, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.tanh, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.tanh, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.tanh, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=tf.tanh, name=\"hidden6\")\n",
    "    logits = tf.layers.dense(hidden6, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Narrowing\n",
    "- 6 hidden + 1 input + 1 output layers\n",
    "- Chose this structure to see how echoing with reduced dimensionality of data affects classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 31\n",
    "n_hidden2 = 25\n",
    "n_hidden3 = 20\n",
    "n_hidden4 = 15\n",
    "n_hidden5 = 10\n",
    "n_hidden6 = 5\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.tanh, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.tanh, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.tanh, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.tanh, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.tanh, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=tf.tanh, name=\"hidden6\")\n",
    "    logits = tf.layers.dense(hidden6, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraction and then Expansion\n",
    "- 7 hidden + 1 input + 1 output layers\n",
    "- First Data contracts for dew layers and then data expands\n",
    "- Chose this structure to see how echoing with reduced dimensionality of data affects classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 20\n",
    "n_hidden2 = 15\n",
    "n_hidden3 = 10\n",
    "n_hidden4 = 2\n",
    "n_hidden5 = 10\n",
    "n_hidden6 = 15\n",
    "n_hidden7 = 20\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=selu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=selu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=selu, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=selu, name=\"hidden6\")\n",
    "    hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=selu, name=\"hidden7\")\n",
    "    logits = tf.layers.dense(hidden7, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 20\n",
    "n_hidden2 = 15\n",
    "n_hidden3 = 10\n",
    "n_hidden4 = 2\n",
    "n_hidden5 = 10\n",
    "n_hidden6 = 15\n",
    "n_hidden7 = 20\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.tanh, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.tanh, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.tanh, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.tanh, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.tanh, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=tf.tanh, name=\"hidden6\")\n",
    "    hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=tf.tanh, name=\"hidden7\")\n",
    "    logits = tf.layers.dense(hidden7, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = XTrain.shape[1]\n",
    "n_hidden1 = 20\n",
    "n_hidden2 = 15\n",
    "n_hidden3 = 10\n",
    "n_hidden4 = 2\n",
    "n_hidden5 = 10\n",
    "n_hidden6 = 15\n",
    "n_hidden7 = 20\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=selu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=selu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=selu, name=\"hidden5\")\n",
    "    hidden6 = tf.layers.dense(hidden5, n_hidden6, activation=selu, name=\"hidden6\")\n",
    "    hidden7 = tf.layers.dense(hidden6, n_hidden7, activation=selu, name=\"hidden7\")\n",
    "    logits = tf.layers.dense(hidden7, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking how testing accuracy after every 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "acc_test = []\n",
    "out = None\n",
    "temp = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        temp.append(sess.run([loss,training_op], feed_dict={X: XTrain, y: TTrain}))\n",
    "        if epoch % 100:\n",
    "            acc_train.append(accuracy.eval(feed_dict={X: XTrain, y: TTrain}))\n",
    "            acc_test.append(accuracy.eval(feed_dict={X: XTest, y: TTest}))\n",
    "    print(\"Test Data Accuracy: \",accuracy.eval(feed_dict={X: XTest, y: TTest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(np.array(temp)[:,0])\n",
    "plt.title(\"Loss Change\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(acc_train)\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(acc_test)\n",
    "plt.title(\"Testing Accuracy\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Accuracy Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Results\n",
    "\n",
    "- Presents the results of applications of your deep networks.\n",
    "- Visualize the results \n",
    "- Discuss about the choice of network structures and performance of it as you change the structures.  \n",
    "- What do you think about the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Discussion: (All for this dataset*****)\n",
    "    - All these structures gave me accuracies that were very close to each other\n",
    "    - Contraction and then Expansion gave the best result, so I will change some parameters and see if value changes\n",
    "    - You see sudden improvement in training accuracy and quick drop in loss as well\n",
    "    - Iterations is not the cause for this accuracy\n",
    "    - If we compare at 500 iteration then we can wee that 2 structures are still trying to learn and improve but other structures just learn very quickly and then become stable\n",
    "    - Adam Optimizer is slow than Gradient Descent\n",
    "        - It fluctuates a lot\n",
    "    - Relu is better than tanh also tested\n",
    "\n",
    "### Results Visualized while running each layer\n",
    "    - Already Done while implementing\n",
    "\n",
    "\n",
    "### Understanding the structure with personal Biases\n",
    "    - Now, I was trying to form structures by some intuional assumptions but I have no idea if that makes sense.\n",
    "    - Idea is that each layer is useful in understanding the data\n",
    "    - Adding more layer means finding some of the combinations of these columns aggregated based on some weights.\n",
    "    - Now, I interpreted reducing number of nodes in a layer means reducing dimentionality and vice versa\n",
    "\n",
    "### My opinions\n",
    "    - I guess these structures are giving very similar results because small number of data points.\n",
    "    - Or it can be some parameters as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Discuss the challenges or somethat that you learned. \n",
    "If you have any suggestion about the assignment, you can write about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Challenges:\n",
    "    - Understanding how flow is created and then how it is executed\n",
    "    - I was trying to implement different types of nodes (memory, rnn, convolutional) but lacked the knowledge to\n",
    "    \n",
    "- Learning:\n",
    "    - Learnt how to create a DNN with feed-forward and back propagation\n",
    "    - Network structures have some meaning behind them\n",
    "    - Increasing iterations doesn't necessarily mean accuracy improves\n",
    "    - Considering the trade offs, take necessary hyper parameters\n",
    "    - Try and test different Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "- Test your program with GPUs and compare training speed. You can try [FloydHub](https://www.floydhub.com/) for free 2 hours of GPU resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "DO NOT forget to submit your data! Your notebook is supposed to run fine after running your codes. \n",
    "\n",
    "\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "15 | Data | \n",
    " | 5| Includes description of your data\n",
    " | 5| Plots to visualize data \n",
    " | 5| Reading and analyzing the plots \n",
    "50 | Methods | \n",
    " | 30| Explanation of the choice for 3 different neural network structures and reason for the selection. (10 points for each) \n",
    " |   | Relate the choice of your data!\n",
    " | 15| Explanation of codes for each structure\n",
    " |  5| Explanation of experimental codes\n",
    "25 | Results | Your Data\n",
    "| 15| plots for results \n",
    "| 10| Discussions about the choice of network structures \n",
    "5 | Conclusions \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
