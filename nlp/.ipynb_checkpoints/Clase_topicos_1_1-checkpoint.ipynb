{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28rA8XO4kzti"
   },
   "source": [
    "#Tokenización o segmentación por tokens\n",
    "\n",
    "### Se puede separar un texto en oraciones, palabras, caracteres o algunos otros criterios.\n",
    "\n",
    "### Usaremos NLTK en los siguientes ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "375Ze6Kt2_Cx"
   },
   "outputs": [],
   "source": [
    "# Para instalarlo por primera vez:\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dghIdSpVlBlG"
   },
   "outputs": [],
   "source": [
    "import nltk  # Python Natural Language Toolkit:  https://www.nltk.org/ \n",
    "nltk.download('punkt')      #  Punkt Sentence Tokenizer:   https://www.kite.com/python/docs/nltk.punkt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhOEmtjy3atA"
   },
   "outputs": [],
   "source": [
    "#nltk.download()  # Cuando lo deees, puedes seleccionar el o los paquetes particulares que desees instalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyF-bBp65_G9"
   },
   "outputs": [],
   "source": [
    "f = open(\"arreola_guardagujas.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGS3LhPm6IMn"
   },
   "outputs": [],
   "source": [
    "cuento = f.read()\n",
    "cuento   # este es un string que contiene todo el documento (cuento) de Arreola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKMIU5pR7aqz",
    "outputId": "5872f6ca-97df-4b0c-f638-80270d49efe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14059"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cuento)  # Total de caracteres del cuento/documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7AyVC1I7wwL"
   },
   "source": [
    "###**Tokenización por enunciados:** \n",
    "\n",
    "###Por default NLTK tokeniza texto en inglés, pero también puede tokenizar otros idiomas como Español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcsAGzIk8LZe"
   },
   "outputs": [],
   "source": [
    "# Podemos tokenizar nuestro corpus en oraciones usando el modelo predeterminado en Español ya integrado:\n",
    "sent = nltk.sent_tokenize(text=cuento, language='spanish')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF9OFtgo8Rez"
   },
   "outputs": [],
   "source": [
    "sent[0:10]    # observa que la separación de las oraciones predeterminada no es por salto de página \"\\n\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q38S5WU8yVr"
   },
   "outputs": [],
   "source": [
    "senten = nltk.sent_tokenize(text=cuento, language='french')    # german, italian, french, portuguese...\n",
    "senten[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmwHBYBVBDYK"
   },
   "source": [
    "Veamos si usando los modelos pre-entrenados con otros idiomas diferentes al español resulta \"exactamente\" la misma división de oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pINncFB4BPAl"
   },
   "outputs": [],
   "source": [
    "print(sent==senten)   # con algunos idiomas diferentes al del documento se pueden obtener diferentes tokenizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEijloWH0ama"
   },
   "source": [
    "### En caso de tener un modelo personal pre-entrenado, lo podríamos utilizar también.\n",
    "\n",
    "###El poder entrenar corpus con idiomas particulares ayuda a identificar mejor las partes de las oraciones, estructuras gramaticales, palabras o caracteres usuales con las que inicia o termina una oración, palbras que por lo general aparecen juntas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1UztdJf5wx0"
   },
   "outputs": [],
   "source": [
    "spanish_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COlnJ2YN-Axy"
   },
   "outputs": [],
   "source": [
    "ss = spanish_tokenizer.tokenize(cuento.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAD09f5y_bce"
   },
   "source": [
    "### **Tokenización por palabras:**\n",
    "\n",
    "Una vez que tienes cada oración, podrás ahora separarlas por \"palabras\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aaUpMxR_ACR"
   },
   "outputs": [],
   "source": [
    "w = nltk.word_tokenize(ss[7], language='spanish')\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n3dCeolHTwg"
   },
   "source": [
    "### Separemos todo el cuento de Arreola en palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-cGgmkdHfRD"
   },
   "outputs": [],
   "source": [
    "sent = nltk.sent_tokenize(text=cuento, language='spanish')   \n",
    "tokens_words = [nltk.word_tokenize(s) for s in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hskbojUVD__h"
   },
   "source": [
    "###Veamos como se lleva a cabo la separación de algunas contracciones en inglés: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBAr4Nma-_-A"
   },
   "outputs": [],
   "source": [
    "r = \"haven't I'll ain't can't everyone's o'clock ol' 'tis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-uJC_Mw-_6H"
   },
   "outputs": [],
   "source": [
    "print(nltk.word_tokenize(r, language='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0XfyLCCZc3w"
   },
   "source": [
    "### **Usando el método split() de string** \n",
    "\n",
    "Cuando cargas o generas un documento separado por renglones de strings, también podrás seguir tokenizándolo bajo diferentes criterios. \n",
    "\n",
    "Veamos algunos ejemolos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fPgLaj_ZvKZ"
   },
   "outputs": [],
   "source": [
    "sent  # tenemos una lista de enunciados (strings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLcIeO4GJhZi"
   },
   "outputs": [],
   "source": [
    "tks1 = []             \n",
    "for linea in sent:\n",
    "  x = str(linea).split('\\n')   # separando por saltos de línea \"\\n\"\n",
    "  tks1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycbc9L4IdeJH"
   },
   "outputs": [],
   "source": [
    "tks2 = []             \n",
    "for linea in sent:\n",
    "  x = str(linea).split(' ')   # separando por espacios en blanco ... en dado caso puedes separar por \",\", \".\", etc...\n",
    "  tks2.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyOqepBPd2K_"
   },
   "outputs": [],
   "source": [
    "tks3 = []             \n",
    "for linea in sent:\n",
    "  x = str(linea).split()   # en este caso separa por espacios en blanco y saltos de línea \"\\n\"\n",
    "  tks3.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oynm3BcJhEYm"
   },
   "outputs": [],
   "source": [
    "print(len(sent))\n",
    "print(len(tks1))\n",
    "print(len(tks2))\n",
    "print(len(tks3))\n",
    "print(len(tks1[0]))\n",
    "print(len(tks2[0]))\n",
    "print(len(tks3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HhxIIITiVB1"
   },
   "source": [
    "### Podemos concatenar también todos los enunciados para obtener un solo documento formado por una lista de tokens (palabras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2JBZMeChy96"
   },
   "outputs": [],
   "source": [
    "u=[]\n",
    "for t in tks3:   # para el caso de una lista de enunciados formado por strings de palabras\n",
    "  u += t\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clase_topicos_1_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
